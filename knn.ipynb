{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2689f0e9",
   "metadata": {},
   "source": [
    "# Baseline k-Nearest Neighbors Classifier\n",
    "\n",
    "We first create a baseline k-Nearest Neighbors classifier, which will serve as a reference for our classification task. We define our hyperparameter grid to search over `n_neighbors` values of 3, 5, and 7, and weights options 'uniform' and 'distance'. We use `GridSearchCV` with 5-fold cross-validation and the 'f1_weighted' scoring metric to tune our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abe299ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    auc,\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "267d93ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b44061b",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = \"results/models\"\n",
    "OUT_VIS = \"results/figures\"\n",
    "OUT_CSV = \"results/csv\"\n",
    "for d in (OUT_DIR, OUT_VIS, OUT_CSV):\n",
    "    os.makedirs(d, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6178fe63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    X_train = pd.read_csv(\"data/processed/X/train.csv\")\n",
    "    X_val = pd.read_csv(\"data/processed/X/val.csv\")\n",
    "    logging.info(f\"Loaded features: X_train {X_train.shape}, X_val {X_val.shape}\")\n",
    "\n",
    "    y_train_df = pd.read_csv(\"data/processed/Y/train.csv\")\n",
    "    y_val_df = pd.read_csv(\"data/processed/Y/val.csv\")\n",
    "    logging.info(f\"Loaded labels: y_train {y_train_df.shape}, y_val {y_val_df.shape}\")\n",
    "\n",
    "    y_train = y_train_df.iloc[:, 0]\n",
    "    y_val = y_val_df.iloc[:, 0]\n",
    "    logging.info(f\"Initial y_train classes: {sorted(y_train.unique())}\")\n",
    "\n",
    "    X_train = X_train.select_dtypes(include=[np.number])\n",
    "    X_val = X_val.select_dtypes(include=[np.number])\n",
    "    logging.info(f\"Numeric filter: X_train {X_train.shape}, X_val {X_val.shape}\")\n",
    "\n",
    "    threshold = 50\n",
    "    counts = y_train.value_counts()\n",
    "    logging.info(f\"Pre-merge class counts: {counts.to_dict()}\")\n",
    "    rare = counts[counts < threshold].index.tolist()\n",
    "    if rare:\n",
    "        y_train = y_train.replace({cls: \"Other\" for cls in rare})\n",
    "        y_val = y_val.replace({cls: \"Other\" for cls in rare})\n",
    "        logging.info(f\"Merged rare classes: {rare} -> 'Other'\")\n",
    "    else:\n",
    "        logging.info(\"No rare classes to merge.\")\n",
    "    logging.info(f\"Post-merge classes: {sorted(y_train.unique())}\")\n",
    "\n",
    "    return X_train, y_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a764717e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 20:53:22,211 INFO Loaded features: X_train (88307, 425), X_val (18923, 425)\n",
      "2025-07-13 20:53:22,223 INFO Loaded labels: y_train (88307, 1), y_val (18923, 1)\n",
      "2025-07-13 20:53:22,230 INFO Initial y_train classes: ['Cell Junction', 'Cell Projection', 'Cell Surface', 'Cytoplasm', 'Endoplasmic Reticulum', 'Endosome', 'Golgi Apparatus', 'Lysosome', 'Membrane', 'Mitochondrion', 'Nucleus', 'Periplasm', 'Peroxisome', 'Plastid', 'Secreted', 'Vacuole', 'Virion']\n",
      "2025-07-13 20:53:22,321 INFO Numeric filter: X_train (88307, 424), X_val (18923, 424)\n",
      "2025-07-13 20:53:22,327 INFO Pre-merge class counts: {'Cytoplasm': 49960, 'Membrane': 16586, 'Secreted': 8934, 'Nucleus': 7277, 'Mitochondrion': 2519, 'Periplasm': 1019, 'Virion': 1007, 'Endoplasmic Reticulum': 308, 'Peroxisome': 159, 'Lysosome': 115, 'Vacuole': 112, 'Plastid': 102, 'Golgi Apparatus': 96, 'Cell Surface': 58, 'Endosome': 48, 'Cell Junction': 6, 'Cell Projection': 1}\n",
      "2025-07-13 20:53:22,344 INFO Merged rare classes: ['Endosome', 'Cell Junction', 'Cell Projection'] -> 'Other'\n",
      "2025-07-13 20:53:22,349 INFO Post-merge classes: ['Cell Surface', 'Cytoplasm', 'Endoplasmic Reticulum', 'Golgi Apparatus', 'Lysosome', 'Membrane', 'Mitochondrion', 'Nucleus', 'Other', 'Periplasm', 'Peroxisome', 'Plastid', 'Secreted', 'Vacuole', 'Virion']\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4372b6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_knn(X_train, y_train):\n",
    "    pipeline = Pipeline([(\"scaler\", StandardScaler()), (\"knn\", KNeighborsClassifier())])\n",
    "    param_grid = {\n",
    "        \"knn__n_neighbors\": [3, 5, 7],\n",
    "        \"knn__weights\": [\"uniform\", \"distance\"],\n",
    "    }\n",
    "    grid = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grid=param_grid,\n",
    "        cv=5,\n",
    "        scoring=\"f1_weighted\",\n",
    "        n_jobs=-1,\n",
    "        verbose=1,\n",
    "        error_score=\"raise\",\n",
    "    )\n",
    "    logging.info(\"Starting k-NN GridSearchCV...\")\n",
    "    grid.fit(X_train, y_train)\n",
    "    logging.info(f\"Best params: {grid.best_params_}\")\n",
    "    logging.info(f\"Best CV F1-weighted: {grid.best_score_:.4f}\")\n",
    "    return grid.best_estimator_, grid.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9236110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_save(model, X, y, out_csv=OUT_CSV, out_vis=OUT_VIS):\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    f1w = f1_score(y, y_pred, average=\"weighted\")\n",
    "    logging.info(f\"Validation accuracy: {acc:.4f}\")\n",
    "    logging.info(f\"Validation F1-weighted: {f1w:.4f}\")\n",
    "\n",
    "    # Save classification report\n",
    "    report = classification_report(y, y_pred, output_dict=True, zero_division=0)\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    report_df.to_csv(f\"{out_csv}/knn_classification_report.csv\")\n",
    "\n",
    "    # Save confusion matrix\n",
    "    classes = model.classes_\n",
    "    cm = confusion_matrix(y, y_pred, labels=classes)\n",
    "    cm_df = pd.DataFrame(cm, index=classes, columns=classes)\n",
    "    cm_df.to_csv(f\"{out_csv}/knn_confusion_matrix.csv\")\n",
    "\n",
    "    # Save ROC data\n",
    "    y_bin = label_binarize(y, classes=classes)\n",
    "    pred_bin = label_binarize(y_pred, classes=classes)\n",
    "    fpr, tpr, roc_auc = {}, {}, {}\n",
    "    for i, cls in enumerate(classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], pred_bin[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        pd.DataFrame({\"fpr\": fpr[i], \"tpr\": tpr[i]}).to_csv(\n",
    "            f\"{out_csv}/knn_roc_curve_{cls}.csv\", index=False\n",
    "        )\n",
    "\n",
    "    # Plot overall ROC\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for i, cls in enumerate(classes):\n",
    "        plt.plot(fpr[i], tpr[i], label=f\"{cls} (AUC={roc_auc[i]:.2f})\")\n",
    "    plt.plot([0, 1], [0, 1], \"k--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"k-NN ROC Curves (One-vs-Rest)\")\n",
    "    plt.legend(loc=\"lower right\", fontsize=\"small\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{out_vis}/knn_roc_curves.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    return acc, f1w, classes, roc_auc, fpr, tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67a794e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_k_values(X_train, y_train, X_val, y_val, ks=[3, 5, 7]):\n",
    "    records = []\n",
    "    for k in ks:\n",
    "        model = Pipeline(\n",
    "            [(\"scaler\", StandardScaler()), (\"knn\", KNeighborsClassifier(n_neighbors=k))]\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "        acc = accuracy_score(y_val, y_pred)\n",
    "        f1w = f1_score(y_val, y_pred, average=\"weighted\")\n",
    "        records.append({\"k\": k, \"accuracy\": acc, \"f1_weighted\": f1w})\n",
    "    df = pd.DataFrame(records)\n",
    "    # Plot\n",
    "    df.set_index(\"k\").plot.bar(rot=0)\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(\"k-NN Benchmark: Accuracy & Weighted-F1 by k\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUT_VIS}/knn_benchmark_ks.png\", dpi=150)\n",
    "    plt.close()\n",
    "    df.to_csv(f\"{OUT_CSV}/knn_benchmark_ks.csv\", index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c6c7c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_subset(model, X, y, classes, out_vis=OUT_VIS):\n",
    "    y_pred = model.predict(X)\n",
    "    rpt = classification_report(y, y_pred, output_dict=True, zero_division=0)\n",
    "    df_rpt = pd.DataFrame(rpt).transpose()\n",
    "    # select two lowest-recall classes with at least 5 samples\n",
    "    df_rpt[\"recall\"] = df_rpt[\"recall\"]\n",
    "    eligible = df_rpt.loc[(df_rpt.index.isin(classes)) & (df_rpt[\"support\"] >= 5)]\n",
    "    worst = eligible.sort_values(\"recall\").head(2).index.tolist()\n",
    "    for cls in worst:\n",
    "        cm = confusion_matrix(\n",
    "            y, y_pred, labels=[cls], normalize=\"true\"\n",
    "        )  # 1x1 matrix? better to show binary: cls vs rest\n",
    "        # create binary confusion\n",
    "        binary_y = (y == cls).astype(int)\n",
    "        binary_pred = (y_pred == cls).astype(int)\n",
    "        cm2 = confusion_matrix(binary_y, binary_pred)\n",
    "        sns.heatmap(\n",
    "            cm2,\n",
    "            annot=True,\n",
    "            fmt=\"d\",\n",
    "            cmap=\"Blues\",\n",
    "            xticklabels=[f\"!{cls}\", cls],\n",
    "            yticklabels=[f\"!{cls}\", cls],\n",
    "        )\n",
    "        plt.title(f\"Binary Confusion: {cls} vs Rest\")\n",
    "        plt.ylabel(\"True\")\n",
    "        plt.xlabel(\"Pred\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{out_vis}/knn_confusion_binary_{cls}.png\", dpi=150)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d073d493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_bottom_roc(roc_auc, fpr, tpr, classes, out_vis=OUT_VIS):\n",
    "    # identify top and bottom\n",
    "    sorted_auc = sorted(roc_auc.items(), key=lambda x: x[1])\n",
    "    bottom = sorted_auc[0][0]\n",
    "    top = sorted_auc[-1][0]\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    for idx in [bottom, top]:\n",
    "        cls = classes[idx]\n",
    "        plt.plot(fpr[idx], tpr[idx], label=f\"{cls} (AUC={roc_auc[idx]:.2f})\")\n",
    "    plt.plot([0, 1], [0, 1], \"k--\")\n",
    "    plt.xlabel(\"FPR\")\n",
    "    plt.ylabel(\"TPR\")\n",
    "    plt.title(\"ROC: Worst vs Best Classes\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{out_vis}/knn_roc_worst_best.png\", dpi=150)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "159c32d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_top5_neighbors(model, X_train, y_train, X_val, sample_idx=None, n_neighbors=5):\n",
    "    if sample_idx is None:\n",
    "        sample_idx = X_val.index[0]\n",
    "    sample_vec = X_val.loc[[sample_idx]]\n",
    "    knn = model.named_steps[\"knn\"]\n",
    "    scaler = model.named_steps[\"scaler\"]\n",
    "    sample_scaled = scaler.transform(sample_vec)\n",
    "    distances, neighbors = knn.kneighbors(sample_scaled, n_neighbors=n_neighbors)\n",
    "    logging.info(f\"Sample index: {sample_idx}\")\n",
    "    for dist, nbr in zip(distances[0], neighbors[0]):\n",
    "        seq_id = X_train.index[nbr]\n",
    "        label = y_train.iloc[nbr]\n",
    "        logging.info(f\"Neighbor: {seq_id}, Label: {label}, Distance: {dist:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb8e1035",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 20:53:22,397 INFO Starting k-NN GridSearchCV...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 20:56:13,562 INFO Best params: {'knn__n_neighbors': 3, 'knn__weights': 'distance'}\n",
      "2025-07-13 20:56:13,564 INFO Best CV F1-weighted: 0.7892\n"
     ]
    }
   ],
   "source": [
    "best_knn, cv_results = train_knn(X_train, y_train)\n",
    "joblib.dump(best_knn, os.path.join(OUT_DIR, \"knn_baseline_best.pkl\"))\n",
    "pd.DataFrame(cv_results).to_csv(f\"{OUT_CSV}/knn_cv_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e82b2ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 20:56:20,435 INFO Validation accuracy: 0.8087\n",
      "2025-07-13 20:56:20,436 INFO Validation F1-weighted: 0.8039\n"
     ]
    }
   ],
   "source": [
    "acc, f1w, classes, roc_auc, fpr, tpr = evaluate_and_save(best_knn, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb185276",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cytronicoder/miniforge3/envs/eslp/lib/python3.13/site-packages/sklearn/metrics/_classification.py:534: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "/Users/cytronicoder/miniforge3/envs/eslp/lib/python3.13/site-packages/sklearn/metrics/_classification.py:534: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "2025-07-13 20:56:55,092 INFO Sample index: 0\n",
      "2025-07-13 20:56:55,093 INFO Neighbor: 62172, Label: Membrane, Distance: 11.433\n",
      "2025-07-13 20:56:55,093 INFO Neighbor: 46286, Label: Membrane, Distance: 14.830\n",
      "2025-07-13 20:56:55,094 INFO Neighbor: 44674, Label: Membrane, Distance: 16.192\n",
      "2025-07-13 20:56:55,094 INFO Neighbor: 82422, Label: Membrane, Distance: 16.492\n",
      "2025-07-13 20:56:55,094 INFO Neighbor: 20952, Label: Membrane, Distance: 16.528\n",
      "2025-07-13 20:56:55,095 INFO All analyses complete.\n"
     ]
    }
   ],
   "source": [
    "df_bench = benchmark_k_values(X_train, y_train, X_val, y_val)\n",
    "plot_confusion_subset(best_knn, X_val, y_val, classes)\n",
    "plot_top_bottom_roc(roc_auc, fpr, tpr, classes)\n",
    "show_top5_neighbors(best_knn, X_train, y_train, X_val)\n",
    "\n",
    "logging.info(\"All analyses complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f00cede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1.17 seconds\n",
      "Prediction throughput: 2436 samples/second\n",
      "Test Accuracy: 0.814\n",
      "Weighted F1-Score: 0.809\n",
      "Minimum per-class F1 for classes ≥5%: 0.440\n",
      "Prediction throughput: 2436 samples/second\n",
      "Test Accuracy: 0.814\n",
      "Weighted F1-Score: 0.809\n",
      "Minimum per-class F1 for classes ≥5%: 0.440\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "X_test = pd.read_csv(\"data/processed/X/test.csv\")\n",
    "y_test = pd.read_csv(\"data/processed/Y/test.csv\").squeeze()\n",
    "X_test = X_test.select_dtypes(include=[np.number])\n",
    "X_test = X_test[X_train.columns]\n",
    "\n",
    "# Measure training time\n",
    "start_train = time.time()\n",
    "best_knn.fit(X_train, y_train)\n",
    "train_time = time.time() - start_train\n",
    "print(f\"Training time: {train_time:.2f} seconds\")\n",
    "\n",
    "# Predict and measure throughput\n",
    "start_pred = time.time()\n",
    "y_test_pred = best_knn.predict(X_test)\n",
    "pred_time = time.time() - start_pred\n",
    "throughput = len(X_test) / pred_time\n",
    "print(f\"Prediction throughput: {throughput:.0f} samples/second\")\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "f1w = f1_score(y_test, y_test_pred, average=\"weighted\")\n",
    "print(f\"Test Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Weighted F1-Score: {f1w:.3f}\")\n",
    "\n",
    "# Per-class F1 for classes >=5% support\n",
    "report = classification_report(y_test, y_test_pred, output_dict=True, zero_division=0)\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "threshold_count = len(y_test) * 0.05\n",
    "selected = report_df.loc[(report_df.index != 'accuracy') & (report_df['support'] >= threshold_count)]\n",
    "min_f1 = selected['f1-score'].min() if not selected.empty else None\n",
    "print(f\"Minimum per-class F1 for classes ≥5%: {min_f1:.3f}\" if min_f1 is not None else \"No classes ≥5% support\")\n",
    "\n",
    "# Save detailed test report\n",
    "report_df.to_csv(f\"{OUT_CSV}/knn_test_classification_report.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eslp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
