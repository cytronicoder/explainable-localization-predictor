{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06ac93c9",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "This is a neat script that downloads UniProt data and extracts subcellular localization annotations for each protein. We'll use this data to later train the Random Forest model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac43b080",
   "metadata": {},
   "source": [
    "## Shell script\n",
    "\n",
    "This script downloads the UniProt data and decompresses it. Takes around 3 minutes to run. We later extract the sequences and re-write them as FASTA ourselves; however, you are more than welcome to use the original FASTA files if you prefer using:\n",
    "\n",
    "```bash\n",
    "wget ftp://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_sprot.fasta.gz\n",
    "gunzip uniprot_sprot.fasta.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af20833",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "set -euo pipefail\n",
    "mkdir -p data/raw\n",
    "cd data/raw\n",
    "\n",
    "echo \"Downloading UniProtKB/Swiss-Prot...\"\n",
    "wget -q ftp://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_sprot.dat.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a8f603",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "echo \"Verifying checksum...\"\n",
    "MD5_DAT_EXPECTED=\"ecfb866a5de8f27497af396735f09b30\"\n",
    "\n",
    "MD5_DAT_ACTUAL=$(md5sum data/raw/uniprot_sprot.dat.gz | cut -d' ' -f1)\n",
    "\n",
    "if [[ \"$MD5_DAT_ACTUAL\" != \"$MD5_DAT_EXPECTED\" ]]; then\n",
    "  echo \"ERROR: DAT checksum mismatch ($MD5_DAT_ACTUAL)\"\n",
    "  exit 1\n",
    "fi\n",
    "\n",
    "echo \"Download and verification complete.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cd612e",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "echo \"Decompressing...\"\n",
    "gunzip -f data/raw/uniprot_sprot.dat.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78677033",
   "metadata": {},
   "source": [
    "## Python module\n",
    "\n",
    "The following Python module contains the code that processes the UniProt data and extracts subcellular localization annotations for each protein. It also splits the data into training, validation, and test sets, and saves them to CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b134a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import pandas as pd\n",
    "from Bio import SwissProt, SeqIO\n",
    "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b499730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths for raw data\n",
    "INPUT_DAT = \"data/raw/uniprot_sprot.dat\"\n",
    "# processed annotation and filtering\n",
    "INPUT_ANN = \"data/processed/annotations.csv\"\n",
    "INPUT_FASTA = \"data/processed/nonredundant.fasta\"\n",
    "\n",
    "# outputs\n",
    "OUTPUT_CSV = \"data/processed/annotations.csv\"\n",
    "OUTPUT_FASTA = \"data/processed/filtered.fasta\"\n",
    "OUTPUT_FEAT = \"data/processed/features.csv\"\n",
    "\n",
    "# split datasets\n",
    "OUTPUT_X_TRAIN = \"data/processed/X/train.csv\"\n",
    "OUTPUT_X_TMP   = \"data/processed/X/tmp.csv\"\n",
    "OUTPUT_X_VAL   = \"data/processed/X/val.csv\"\n",
    "OUTPUT_X_TEST  = \"data/processed/X/test.csv\"\n",
    "\n",
    "# split labels\n",
    "OUTPUT_Y_TRAIN = \"data/processed/Y/train.csv\"\n",
    "OUTPUT_Y_TMP   = \"data/processed/Y/tmp.csv\"\n",
    "OUTPUT_Y_VAL   = \"data/processed/Y/val.csv\"\n",
    "OUTPUT_Y_TEST  = \"data/processed/Y/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df244990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create processed output directories\n",
    "for path in [\n",
    "    OUTPUT_CSV, OUTPUT_FASTA, OUTPUT_FEAT,\n",
    "    OUTPUT_X_TRAIN, OUTPUT_X_TMP, OUTPUT_X_VAL, OUTPUT_X_TEST,\n",
    "    OUTPUT_Y_TRAIN, OUTPUT_Y_TMP, OUTPUT_Y_VAL, OUTPUT_Y_TEST,\n",
    "    ]:\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25719cf9",
   "metadata": {},
   "source": [
    "We manually exclude a few terms that indicates non-experimental evidence or are not specific enough to be useful for localization prediction. Additionally, we map some specific biological locations to more general terms to reduce the number of unique labels. This should be extended as needed, as this list is far from exhaustive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954b2ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "_EXCLUDE_TERMS = {\"probable\", \"potential\", \"by similarity\", \"prediction\"}\n",
    "ALLOWED_LOCS = {\n",
    "    \"Cytoplasm\",\n",
    "    \"Nucleus\",\n",
    "    \"Secreted\",\n",
    "    \"Mitochondrion\",\n",
    "    \"Periplasm\",\n",
    "    \"Virion\",\n",
    "    \"Plastid\",\n",
    "    \"Membrane\",\n",
    "    \"Peroxisome\",\n",
    "    \"Endoplasmic Reticulum\",\n",
    "    \"Golgi Apparatus\",\n",
    "    \"Lysosome\",\n",
    "    \"Vacuole\",\n",
    "    \"Cell Projection\",\n",
    "    \"Cell Surface\",\n",
    "    \"Cell Junction\",\n",
    "    \"Endosome\",\n",
    "}\n",
    "_SYNONYM_MAP = {\n",
    "    \"host membrane\": \"Membrane\",\n",
    "    \"host cell membrane\": \"Membrane\",\n",
    "    \"cell membrane\": \"Membrane\",\n",
    "    \"cell outer membrane\": \"Membrane\",\n",
    "    \"plasma membrane\": \"Membrane\",\n",
    "    \"mitochondrial matrix\": \"Mitochondrion\",\n",
    "    \"mitochondrion matrix\": \"Mitochondrion\",\n",
    "    \"endoplasmic reticulum lumen\": \"Endoplasmic Reticulum\",\n",
    "    \"er lumen\": \"Endoplasmic Reticulum\",\n",
    "    \"golgi\": \"Golgi Apparatus\",\n",
    "    # extend as needed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24005557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _clean_and_primary(text: str) -> str:\n",
    "    text = re.split(r\"Note=\", text, maxsplit=1)[0]\n",
    "    text = re.sub(r\"\\{.*?\\}|\\(.*?\\)\", \"\", text)\n",
    "    part = re.split(r\"[.;]\", text, maxsplit=1)[0].strip()\n",
    "    if not part:\n",
    "        return \"\"\n",
    "    low = part.lower()\n",
    "    if low in _SYNONYM_MAP:\n",
    "        canon = _SYNONYM_MAP[low]\n",
    "    else:\n",
    "        canon = part.title()\n",
    "    return canon if canon in ALLOWED_LOCS else \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08fed63",
   "metadata": {},
   "source": [
    "We exclude multi-compartment entries for this iteration. Single-label classifiers canâ€™t handle proteins annotated to two or more compartments - splitting multi-labels naively can inflate class counts and introduce bias.\n",
    "\n",
    "Some more considerations:\n",
    "\n",
    "+ Performance benchmarks (speed, memory) become harder to interpret when outputs are vectors rather than one label.\n",
    "+ Decision-support tools often struggle to map multi-compartment calls to single ACMG evidence codes.\n",
    "+ Rare two-compartment combinations will have very few examples. This can undermine learning.\n",
    "\n",
    "Future iterations can revisit multi-label approaches once the single-label pipeline produces a good benchmark, as clinically, mis- or multi-localization can be disease-relevant, and it is always good to retain them and preserve for downstream pathway analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114e7ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_protein_data(dat_file: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Parse a UniProt .dat file and return only entries with exactly one\n",
    "    experimentally-verified subcellular location from ALLOWED_LOCS.\n",
    "    \"\"\"\n",
    "    results: List[Dict[str, Any]] = []\n",
    "\n",
    "    try:\n",
    "        handle = open(dat_file)\n",
    "    except OSError as e:\n",
    "        raise RuntimeError(f\"Cannot open file {dat_file}: {e}\")\n",
    "\n",
    "    with handle:\n",
    "        for rec in SwissProt.parse(handle):\n",
    "            locs: List[str] = []\n",
    "\n",
    "            # 1) try structured API\n",
    "            if hasattr(rec, \"subcellular_locations\") and rec.subcellular_locations:\n",
    "                for loc_tuple in rec.subcellular_locations:\n",
    "                    loc = loc_tuple.location or \"\"\n",
    "                    cleaned = _clean_and_primary(loc)\n",
    "                    if cleaned:\n",
    "                        locs.append(cleaned)\n",
    "            else:\n",
    "                # 2) fallback to scanning comments\n",
    "                for comment in rec.comments:\n",
    "                    if not comment.upper().startswith(\"SUBCELLULAR LOCATION:\"):\n",
    "                        continue\n",
    "                    body = comment.split(\":\", 1)[1]\n",
    "                    for piece in re.split(r\"[;]\", body):\n",
    "                        cleaned = _clean_and_primary(piece)\n",
    "                        if cleaned:\n",
    "                            locs.append(cleaned)\n",
    "\n",
    "            # exclude non-experimental evidence\n",
    "            combined = \" \".join(rec.comments).lower()\n",
    "            if any(term in combined for term in _EXCLUDE_TERMS):\n",
    "                continue\n",
    "\n",
    "            # dedupe and require exactly one compartment\n",
    "            unique = list(dict.fromkeys(locs))\n",
    "            if len(unique) != 1:\n",
    "                continue\n",
    "\n",
    "            results.append(\n",
    "                {\n",
    "                    \"entry_name\": rec.entry_name,\n",
    "                    \"sequence\": rec.sequence,\n",
    "                    \"localization\": unique[0],\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3e109c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(OUTPUT_CSV):\n",
    "    df = pd.read_csv(OUTPUT_CSV)\n",
    "    print(f\"Loaded existing annotations from {OUTPUT_CSV}\")\n",
    "else:\n",
    "    print(f\"No existing annotations found, extracting from {INPUT_DAT}\")\n",
    "    print(\"Extracting protein data...\")\n",
    "    protein_data = extract_protein_data(INPUT_DAT)\n",
    "\n",
    "    df = pd.DataFrame(protein_data)\n",
    "    print(f\"Data shape: {df.shape}\")\n",
    "\n",
    "    df.to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f\"\\nWrote annotations to {OUTPUT_CSV}!\")\n",
    "    print(f\"DataFrame saved with {len(df)} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b680663",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nDataset statistics:\")\n",
    "print(f\"Total entries: {len(df)}\")\n",
    "print(f\"Unique localizations: {df['localization'].nunique()}\")\n",
    "print(f\"Average sequence length: {df['sequence'].str.len().mean():.1f}\")\n",
    "\n",
    "print(f\"\\nTop 10 most common localizations:\")\n",
    "print(df[\"localization\"].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac088c5d",
   "metadata": {},
   "source": [
    "The following code is for debugging purposes and can be removed. It simply saves the full localization distribution to a text file for later analysis. This is useful to understand the distribution of localizations in the dataset, and to ensure that the filtering is working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3070fde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/localization_distribution.txt\", \"w\") as f:\n",
    "    f.write(\"Full localization distribution:\\n\")\n",
    "    f.write(df[\"localization\"].value_counts().to_string())\n",
    "    print(\n",
    "        f\"\\nFull localization distribution saved to data/localization_distribution.txt\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923349a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization: convert string columns to categorical to save memory\n",
    "df['entry_name'] = df['entry_name'].astype('category')\n",
    "df['localization'] = df['localization'].astype('category')\n",
    "\n",
    "print(f\"Optimized memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a546fd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nQuick data exploration:\")\n",
    "print(f\"Sequence length distribution:\")\n",
    "print(df['sequence'].str.len().describe())\n",
    "\n",
    "print(f\"\\nData quality checks:\")\n",
    "print(f\"Entries with very short sequences (<50 AA): {(df['sequence'].str.len() < 50).sum()}\")\n",
    "print(f\"Entries with very long sequences (>2000 AA): {(df['sequence'].str.len() > 2000).sum()}\")\n",
    "\n",
    "print(f\"\\nSample of processed data:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32ea627",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OUTPUT_FASTA, \"w\") as out:\n",
    "    for _, row in df.iterrows():\n",
    "        header = f\">{row['entry_name']}|{row['localization']}\"\n",
    "        seq = row[\"sequence\"]\n",
    "        out.write(f\"{header}\\n\")\n",
    "\n",
    "        for i in range(0, len(seq), 80):\n",
    "            out.write(seq[i : i + 80] + \"\\n\")\n",
    "\n",
    "print(f\"Wrote FASTA to {OUTPUT_FASTA}\")\n",
    "print(f\"Generated {len(df)} sequences in FASTA format\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3f62b1",
   "metadata": {},
   "source": [
    "We now use [CD-HIT](https://github.com/weizhongli/cdhit/) to cluster the UniProtKB/Swiss-Prot FASTA file at 90% sequence identity, which is a common practice to reduce redundancy in protein datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894c0036",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "INPUT_FASTA=\"data/processed/filtered.fasta\"\n",
    "OUTPUT_FASTA=\"data/processed/nonredundant.fasta\"\n",
    "THREADS=4\n",
    "\n",
    "echo \"Running CD-HIT...\"\n",
    "cd-hit -i \"$INPUT_FASTA\" \\\n",
    "       -o \"$OUTPUT_FASTA\" \\\n",
    "       -c 0.90 -n 5 \\\n",
    "       -M 16000 -T $THREADS\n",
    "\n",
    "if command -v cd-hit &> /dev/null; then\n",
    "    echo \"CD-HIT is installed, proceeding with clustering...\"\n",
    "    cd-hit -i \"$INPUT_FASTA\" \\\n",
    "       -o \"$OUTPUT_FASTA\" \\\n",
    "       -c 0.90 -n 5 \\\n",
    "       -M 16000 -T $THREADS\n",
    "    echo \"Clustering completed, nonredundant FASTA at $OUTPUT_FASTA\"\n",
    "else\n",
    "    echo \"CD-HIT is not installed. Please install it to perform clustering.\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8353235",
   "metadata": {},
   "source": [
    "### Feature engineering\n",
    "\n",
    "We use the following features for the Random Forest model:\n",
    "\n",
    "+ **Amino acid composition**: The percentage of each standard amino acid in the protein sequence.\n",
    "+ **Dipeptide frequencies**: The frequency of each dipeptide (pair of amino acids) in the protein sequence.\n",
    "+ **GRAVY**: The grand average of hydropathy (GRAVY) of the protein sequence, which is a measure of hydrophobicity.\n",
    "+ **pI**: The isoelectric point of the protein sequence, which is the pH at which the protein has no net charge.\n",
    "+ **Sequence length**: The length of the protein sequence.\n",
    "+ **Original length**: The length of the original protein sequence (before cleaning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e91f459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_matrix(fasta_path: str, ann_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parse a UniProt FASTA and annotation CSV to compute sequence features\n",
    "    (amino-acid composition, dipeptide frequencies, GRAVY, pI).\n",
    "    \"\"\"\n",
    "    # Load annotation map\n",
    "    ann = pd.read_csv(ann_path).set_index(\"entry_name\")[\"localization\"].to_dict()\n",
    "\n",
    "    # Standard amino acids & dipeptides\n",
    "    standard_aas = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "    all_dipeps = [a + b for a in standard_aas for b in standard_aas]\n",
    "\n",
    "    rows = []\n",
    "    skipped = 0\n",
    "\n",
    "    for rec in SeqIO.parse(fasta_path, \"fasta\"):\n",
    "        # parse from the full description (captures spaces)\n",
    "        try:\n",
    "            entry, header_loc = rec.description.split(\"|\", 1)\n",
    "            header_loc = header_loc.strip()\n",
    "        except ValueError:\n",
    "            print(f\"Warning: unexpected header format '{rec.description}'\")\n",
    "            continue\n",
    "\n",
    "        loc = ann.get(entry)\n",
    "        if loc is None:\n",
    "            print(f\"Warning: no annotation for {entry}\")\n",
    "            continue\n",
    "        if loc != header_loc:\n",
    "            print(f\"Warning: header says {header_loc}, ann says {loc}\")\n",
    "\n",
    "        # sequence cleaning\n",
    "        original_seq = str(rec.seq)\n",
    "        cleaned = \"\".join([aa for aa in original_seq.upper() if aa in standard_aas])\n",
    "        if len(cleaned) < 10:\n",
    "            print(f\"Skipping {entry}: cleaned length = {len(cleaned)}\")\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        # feature dict\n",
    "        feats = {\n",
    "            \"entry\": entry,\n",
    "            \"localization\": loc,\n",
    "            \"sequence_length\": len(cleaned),\n",
    "            \"original_length\": len(original_seq),\n",
    "        }\n",
    "\n",
    "        # amino-acid composition + physico-chemical\n",
    "        try:\n",
    "            pa = ProteinAnalysis(cleaned)\n",
    "            comp = { aa: pct/100 for aa, pct in pa.amino_acids_percent.items() }\n",
    "            feats.update(comp)\n",
    "            feats[\"gravy\"] = pa.gravy()\n",
    "            feats[\"pI\"] = pa.isoelectric_point()\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: feature calc failed for {entry}: {e}\")\n",
    "            for aa in standard_aas:\n",
    "                feats[aa] = 0.0\n",
    "            feats.update({\"gravy\": None, \"pI\": None})\n",
    "\n",
    "        # dipeptide freqs (fixed key set)\n",
    "        dipep = dict.fromkeys(all_dipeps, 0)\n",
    "        for i in range(len(cleaned) - 1):\n",
    "            dp = cleaned[i : i + 2]\n",
    "            dipep[dp] += 1\n",
    "        total = sum(dipep.values())\n",
    "        if total > 0:\n",
    "            for dp in dipep:\n",
    "                dipep[dp] /= total\n",
    "\n",
    "        feats.update({f\"dp_{dp}\": freq for dp, freq in dipep.items()})\n",
    "        rows.append(feats)\n",
    "\n",
    "    print(f\"Processed {len(rows)} sequences, skipped {skipped}.\")\n",
    "    df = pd.DataFrame(rows).fillna(0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82539231",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = build_feature_matrix(INPUT_FASTA, INPUT_ANN)\n",
    "df.to_csv(OUTPUT_FEAT, index=False)\n",
    "print(f\"Wrote features to {OUTPUT_FEAT}\")\n",
    "print(f\"Feature matrix shape: {df.shape}\")\n",
    "print(f\"Features per sequence: {df.shape[1] - 2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ab5d0b",
   "metadata": {},
   "source": [
    "For the purposes of this project, we will only stratify the first split to preserve overall class balance in the training set. We will use a plain random split (no stratify) when dividing the held-out data into validation and test sets.\n",
    "\n",
    "**Why not stratify the held-out data?** Even after removing global singletons, we can still end up with some classes that have only one member in the temporary pool. Therefore, a stratified split there will always fail for any class with fewer than 2 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5a6ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(OUTPUT_FEAT)\n",
    "y = df.pop(\"localization\")\n",
    "\n",
    "X_train, X_tmp, y_train, y_tmp = train_test_split(\n",
    "    df, y, test_size=0.30, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_tmp, y_tmp, test_size=0.50, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train dist:\\n\", y_train.value_counts(normalize=True), \"\\n\")\n",
    "print(\"Val dist (approx):\\n\", y_val.value_counts(normalize=True), \"\\n\")\n",
    "print(\"Test dist (approx):\\n\", y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56e4490",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(OUTPUT_X_TRAIN, index=False)\n",
    "X_tmp.to_csv(OUTPUT_X_TMP, index=False)\n",
    "X_val.to_csv(OUTPUT_X_VAL, index=False)\n",
    "X_test.to_csv(OUTPUT_X_TEST, index=False)\n",
    "\n",
    "y_train.to_csv(OUTPUT_Y_TRAIN, index=False)\n",
    "y_tmp.to_csv(OUTPUT_Y_TMP, index=False)\n",
    "y_val.to_csv(OUTPUT_Y_VAL, index=False)\n",
    "y_test.to_csv(OUTPUT_Y_TEST, index=False)\n",
    "\n",
    "print(f\"Split data into train/val/test sets:\")\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
    "print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "print(f\"Data written to {OUTPUT_X_TRAIN}, {OUTPUT_Y_TRAIN}, etc.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
